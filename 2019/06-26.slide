I wrote my own programming language and you can too!

26 Mar 2019
Tags: interpreter, golang, ast, lexer, parser

David Bond
Software Engineer, CRM
david.bond@ovoenergy.com
http://davidsbond.github.io/
@DaviiBond

* What I made:

A simple, interpreted programming language, written in Go.

  func main() {
    try {
      results := await {
        add(1, 2)
        add(3, 4)
        add(5, 6)
      }

      // Print the results
      print(results[0], results[1], results[2])
    } catch(err) {
      print(err)
    }
  }

  async func add(x, y) {
    return x + y
  }

.link https://github.com/davidsbond/zap

: Language works more or less exactly as JavaScript does
: Supports, async/await, try/catch, control flow, loops, functions as first class values, comments, builtins etc.

* What is an interpreter?

Nearly all interpreters boil down to three layers:

- The lexer
- The parser
- The evaluator

: An interpreted language is a type of programming language for which most of its implementations execute instructions directly and freely, without previously compiling a program into machine-language instructions. 
: The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines, and then into another language (often machine code).
: Popular example is JavaScript, with different interpreter implementations V8 (Chrome, C++) and SpiderMonkey (Mozilla, C/C++)

* Implementing a lexer

Main responsibilities of a lexer:

- Read source code character by character
- Convert characters into lexemes
- Convert lexemes into tokens

: The lexer (lexical analyzer or tokenizer) is a program that breaks down the input source code into a sequence of lexemes. 
: It reads the input source code character by character, recognizes the lexemes and outputs a sequence of tokens describing the lexemes.
: A lexeme is a single identifiable sequence of characters (keywords: func, var, for), literals (numbers, strings etc.), identifiers, operators, or punctuation characters ({, (, and .).
: A token is an object describing the lexeme. 
: A token has a type (e.g. Keyword, Identifier, Number, or Operator) and a value (the actual characters of the described lexeme). 
: A token can also contain other information such as the line and column numbers where the lexeme was encountered in the source code.

* 

What this looks like, is a huge `switch` statement

  char := string(l.currentChar)

  switch char {
    case '(':
      // Tokens are stored in constants
      tok = l.newToken(token.LPAREN, char)
    case ')':
      tok = l.newToken(token.RPAREN, char)
    case ',':
      tok = l.newToken(token.COMMA, char)
    case '=':
      // Check if we're assigning (=) or comparing (==)
      if l.peekChar() == '=' {
        l.readChar()
        tok = l.newToken(token.EQ, char)
      } else {
        tok = l.newToken(token.ASSIGN, char)
      }
    // And so on for all possible tokens we want to use...
  }

.link https://github.com/davidsbond/zap/blob/master/lexer/lexer.go source

: Talk about all source code being samples, links to source files will be provided for each snipped