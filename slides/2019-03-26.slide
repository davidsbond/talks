I wrote my own programming language and you can too!

26 Mar 2019

David Bond
Software Engineer
davidsbond93@gmail.com
http://davidsbond.github.io/


* What I made:

A simple, interpreted programming language, written in Go.

  func fib(n) {
    if (n <= 1) {
      return n
    }

    return fib(n - 1) + fib(n - 2)
  }

  fib(9)
  // 34

.link https://github.com/davidsbond/language

: Language works more or less exactly as JavaScript does
: Supports, async/await, control flow, functions as first class values, comments, built-in functions etc.

* Why use go?

- Garbage collection
- Simple concurrency model
- File streaming
- The `rune` type

: Ability to use channels

* What is an interpreter?

Nearly all interpreters boil down to three layers:

- The lexer
- The parser
- The evaluator

: An interpreted language is a type of programming language for which most of its implementations execute instructions directly and freely, without previously compiling a program into machine-language instructions. 
: The interpreter executes the program directly, translating each statement into a sequence of one or more subroutines, and then into another language (often machine code).
: Popular example is JavaScript, with different interpreter implementations V8 (Chrome, C++) and SpiderMonkey (Mozilla, C/C++)

* Implementing the lexer

Main responsibilities of a lexer:

- Read characters from source
- Convert characters into lexemes
- Convert lexemes into tokens
- Reporting errors

: The lexer (lexical analyzer or tokenizer) is a program that breaks down the input source code into a sequence of lexemes. 
: A lexeme is a single identifiable sequence of characters (keywords: func, var, for), literals (numbers, strings etc.), identifiers, operators, or punctuation characters ({, (, and .).
: It reads the input source code character by character, recognizes the lexemes and outputs a sequence of tokens describing the lexemes.

* The lexer struct 

  // The Lexer type breaks down the input source code into a sequence of lexemes and produces
  // tokens.
  Lexer struct {
    // The input stream
    input   *bufio.Reader

    // The current character under inspection
    current rune

    // The current line & column number
    column  int
    line    int
  }

: Package bufio implements buffered I/O. It wraps an io.Reader or io.Writer object, creating another object (Reader or Writer) that also implements the interface but provides buffering and some help for textual I/O.

* Reading characters

  func (l *Lexer) readRune() (err error) {
    // Get the next character from the input stream
    ch, _, err := l.input.ReadRune()

    // If we've hit the end of the file, set the character
    // to zero
    if err == io.EOF {
      ch = 0
    }

    // Reset column and increment line counts if we've hit
    // a newline
    if ch == '\n' {
      l.column = 1
      l.line++
    }

    l.column++
    l.current = ch

    return
  }

* Inspecting future characters

  func (l *Lexer) peekRune() (ch rune, err error) {
    // Get the next input from the stream
    ch, _, err = l.input.ReadRune()

    if err != nil {
      return
    }

    // If all was successful, unread the last
    // character to reset the stream position
    err = l.input.UnreadRune()
    return
  }

* Defining tokens

  const (
    // VAR is the token type used when declaring a mutable
    // variable
    VAR = "var"

    // ASSIGN is the token type used for assignment statements
    ASSIGN = "="

    // EQUALS is the token type used for equality statements.
    EQUALS = "=="

    // NOTEQ is the token type used for not equals operators
    NOTEQ = "!="

    // BANG is the token type used for not expressions.
    BANG = "!"

    // STRING is the token type used for opening/closing literal strings.
    STRING = `"`
  )

: A token is an object describing the lexeme. 
: A token has a type (e.g. Keyword, Identifier, Number, or Operator) and a value (the actual characters of the described lexeme). 
: A token can also contain other information such as the line and column numbers where the lexeme was encountered in the source code.

* Turning lexemes into tokens

  func (l *Lexer) NextToken() (tok *token.Token, err error) {
    l.skipWhitespace()

    switch l.current {
    case '<':
      tok = token.New(token.LT, token.LT, l.line, l.column)
    case '>':
      tok = token.New(token.GT, token.GT, l.line, l.column)
    case '*':
      tok = token.New(token.ASTERISK, token.ASTERISK, l.line, l.column)
    case '%':
      tok = token.New(token.MOD, token.MOD, l.line, l.column)
    case '(':
      tok = token.New(token.LPAREN, token.LPAREN, l.line, l.column)
    case ')':
      tok = token.New(token.RPAREN, token.RPAREN, l.line, l.column)
    case ',':
      tok = token.New(token.COMMA, token.COMMA, l.line, l.column)
    case '{':
      tok = token.New(token.LBRACE, token.LBRACE, l.line, l.column)
    // Add more tokens here
    }
  }

* Using runes

  bytes := []byte("日abc") 
  runes := []rune("日abc")

  fmt.Println(bytes) // [230 151 165 97 98 99]
  fmt.Println(runes) // [26085 97 98 99]

: The rune type is an alias for int32, and is used to emphasize that an integer represents a code point.
: For example, in the string "日abc" the character 日 (code point 26085) is encoded using three bytes, while the ASCII characters a, b and c (code points 97, 98 and 99) only use one
: Saves additional logic when iterating over characters. Allows expanding valid syntax with UTF-8 symbols.
: This kind of functionality is very useful if you want to include mathematical language (like Julia)

* Lessons from implementing a lexer

- Avoid tokens for whitespace & newlines
- Use constants

: Using newlines/whitespace as tokens generates a lot of boilerplate to handle/ignore them elsewhere
: Central variables for token identifiers makes changes simple

* Implementing the parser

The parser is responsible for:

- Building the AST
- Operator precedence
- Understanding prefixes, infixes & postfixes
- Reporting errors

: An AST is a tree representation of the abstract syntactic structure of source code written in a programming language. 
: Each node of the tree denotes a construct occurring in the source code (if statement, for loop etc.)
: The syntax is "abstract" in the sense that it does not represent every detail appearing in the real syntax, but rather just the structural, content-related details.
: Operator precedence is just BIDMAS for logical/binary operators
: Infix, Postfix and Prefix notations are three different but equivalent ways of writing expressions.

* The parser struct

  type (
    // The Parser type is responsible for iterating over tokens provided by the lexer
    // and converting them into the abstract syntax tree
    Parser struct {
      // The lexer, for reading tokens
      lexer        *lexer.Lexer

      // The current and next tokens
      currentToken *token.Token
      peekToken    *token.Token

      // Maps for storing the parsing functions to be used for different
      // tokens
      prefixParsers  map[token.Type]prefixParseFn
      infixParsers   map[token.Type]infixParseFn
      postfixParsers map[token.Type]postfixParseFn

      // Any errors we found
      errors []error
    }
  )

* Iterating over tokens

  func (p *Parser) nextToken() {
    var err error

    p.currentToken = p.peekToken
    p.peekToken, err = p.lexer.NextToken()

    if err != nil && err != io.EOF {
      p.errors = append(p.errors, err)
    }
  }

* The parser entrypoint

  func (p *Parser) Parse() (*ast.AST, []error) {
    ast := &ast.AST{}

    // While we're not at the end of the file, parse
    // statements and append them to the AST.
    for !p.curTokenIs(token.EOF) {
      stmt := p.parseStatement()

      if stmt != nil {
        ast.Nodes = append(ast.Nodes, stmt)
      }

      p.nextToken()
    }

    return ast, p.errors
  }

* Defining types that represent source code

  type (
    // The IfExpression type represents an if statement in the source code. It consists
    // of a condition, consequence and optional consequence.
    // For example:
    // if (a == 0) { <-- Condition
    //   return -1   <-- Consequence
    // } else {
    //   return a    <-- Alternative
    // }
    IfExpression struct {
      Token       *token.Token
      Condition   Node
      Consequence *BlockStatement
      Alternative *BlockStatement
    }
  )

* Parsing a variable declaration 

  func (p *Parser) parseVarStatement() *ast.VarStatement {
    stmt := &ast.VarStatement{Token: p.currentToken}

    if !p.expectPeek(token.IDENT) {
      return nil
    }

    stmt.Name = &ast.Identifier{
      Token: p.currentToken,
      Value: p.currentToken.Literal,
    }

    if !p.expectPeek(token.ASSIGN) {
      return nil
    }

    p.nextToken()
    stmt.Value = p.parseExpression(LOWEST)

    return stmt
  }

* Prefix expressions

- Operators are written before their operands (!, -)
- Operators act on the nearest value to the right
- Evaluated left-to-right
- Also known as "Polish notation"

: Although Prefix "operators are evaluated left-to-right", they use values to their right, and if these values themselves involve computations then this changes the order that the operators have to be evaluated in
: The description "Polish" refers to the nationality of logician Jan Łukasiewicz (yan oo ka sheh witz)
: Invented in 1924

* Infix expressions

- Operators are written in-between their operands (+, *, ...)
- Need a defined operator precedence
- Initially evaluated left-to-right

: Notation commonly used in arithmetical and logical formulae and statements
: Infix notation is more difficult to parse by computers than prefix notation (e.g. + 2 2) or postfix notation (e.g. 2 2 +). 
: However many programming languages use it due to its familiarity. It is more used in arithmetic, e.g. 5 × 6

* Defining precedence

  const (
    _ int = iota
    LOWEST
    EQUALS
    LESSGREATER
    SUM
    PRODUCT
    PREFIX
    CALL
    INDEX
  )

  var (
    precedence = map[token.Type]int{
      token.COMMENT:  LOWEST,
      token.EQUALS:   EQUALS,
      token.PLUS:     SUM,
      token.LT:       LESSGREATER,
      token.ASTERISK: PRODUCT,
      token.LPAREN:   CALL,
      token.ASSIGN:   INDEX,
    }
  )

* Postfix expressions

- Operators are written after their operands (++, --)
- Evaluated left-to-right
- Proposed in 1954 by Arthur Burks, Don Warren, and Jesse Wright
- Also known as "Reverse Polish notation"

: Was independently reinvented by Friedrich L. Bauer and Edsger W. Dijkstra in the early 1960s to reduce computer memory access and utilize the stack to evaluate expressions.
: Reverse Polish has been found to lead to faster calculations
: Because reverse Polish calculators do not need expressions to be parenthesized, fewer operations need to be entered to perform typical calculations

* Parsing pre/post/in-fix

    prefix, ok := p.prefixParsers[p.currentToken.Type]

    if !ok {
      p.error("%s is not supported as a prefix", p.currentToken.Literal)
      return nil
    }

    leftExp := prefix()

    for precedence < p.peekPrecedence() {
      infix := p.infixParsers[p.peekToken.Type]

      if infix == nil {
        return leftExp
      }

      p.nextToken()
      leftExp = infix(leftExp)
    }

    if postfix, ok := p.postfixParsers[p.peekToken.Type]; ok {
      leftExp = postfix(leftExp)
    }

* Lessons from implementing a parser

- Start with a small, sensible feature set  
- Be mindful when iterating over tokens
- Make debugging easier for yourself

* Implementing the evaluator

The evaluator is responsible for:

- Creating & comparing objects
- Managing scope
- Reading the AST
- Built-in methods

* Objects

  type (
    // The Object interface defines behavior for all objects.
    Object interface {
      Type() Type
      String() string
      Clone() Object
    }

    // The Type type contains an object's type.
    Type string
  )

* Defining types of objects

  const (
    // TypeString is the type for a string value.
    TypeString = "String"
  )

  type (
    // The String type represents a string value in memory.
    String struct {
      Value string
    }
  )

  // Type returns the type of the object.
  func (str *String) Type() Type {
    return TypeString
  }

* Managing scope

  type (
    // The Scope type contains all accessible objects for a certain node.
    Scope struct {
      objects map[string]Object
      parent  *Scope
    }
  )

  // NewChildScope creates a new scope using the called scope as the
  // parent.
  func (s *Scope) NewChildScope() *Scope {
    scp := NewScope()
    scp.parent = s

    return scp
  }

: The scope is just a map containing all objects available to an expression.
: Scopes are linked, parent to child. For example, a function call will use a child scope from the calling scope
: Provides functionality like variables exclusive to functions (parameters) or loops.
: Using this scope model means you can reuse names of objects from the parent scope without overriding them
: Literally just a map

* Reading the AST

  func evalAST(ast *ast.AST, scope *object.Scope) object.Object {
    var result object.Object

    for _, node := range ast.Nodes {
      result = Evaluate(node, scope)

      if result != nil && result.Type() == object.TypeError {
        break
      }

      switch result := result.(type) {
      case *object.ReturnValue:
        return result.Value
      }
    }

    return result
  }

* Evaluating expressions

  func ifExpression(node *ast.IfExpression, scope *object.Scope) object.Object {
    var result object.Object
    condition := Evaluate(node.Condition, scope)

    if isError(condition) {
      return condition
    }

    if condition == TRUE {
      result = Evaluate(node.Consequence, scope.NewChildScope())
    }

    if condition == FALSE && node.Alternative != nil {
      result = Evaluate(node.Alternative, scope.NewChildScope())
    }

    return result
  }

* Built-in methods

- Identifiers that call logic in the interpreter when validated
- `len` (Go), `typeOf` (JS), `compile` (Python) etc...

  type (
    // The Builtin type represents a built-in function that can be called
    // from the source code.
    Builtin func(args ...Object) Object
  )

* Implementing len()

  // Len returns the length of a given string or array. Or the number of key
  // in a hash.
  func Len(args ...object.Object) object.Object {
    if len(args) > 1 || len(args) == 0 {
      return object.Error("built-in 'len' function only takes one argument")
    }

    switch obj := args[0].(type) {
    default:
      return object.Error("built-in 'len' does not support type %s", obj.Type())
    case *object.Array:
      return &object.Number{Value: float64(len(obj.Elements))}
    case *object.String:
      return &object.Number{Value: float64(len(obj.Value))}
    }
  }

: Demo on len function

* Built-in values

- Using the `rune` type, we can make arbitrary characters have values

: Demo on infinity/pi

* Wrapping up
